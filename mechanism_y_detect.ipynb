{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanism Y - Pattern Detection Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Imports and Setup
",
    "from pyspark.sql import SparkSession
",
    "from pyspark.sql.functions import *
",
    "from pyspark.sql.window import Window
",
    "
",
    "spark = SparkSession.builder.appName("Mechanism Y - Detection").getOrCreate()
",
    "
",
    "# Schema to be reused (adjust to match your data)
",
    "input_path = "s3://your-s3-bucket/transactions_batch/"
",
    "importance_path = "dbfs:/FileStore/CustomerImportance.csv"
"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load Importance Data
",
    "importance_df = spark.read.option("header", True).csv(importance_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Stream Read Transactions
",
    "schema = spark.read.option("header", True).csv(input_path).schema
",
    "streaming_df = spark.readStream.schema(schema).option("header", True).csv(input_path)
"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîé Define Pattern Detection Logic
",
    "def process_batch(batch_df, batch_id):
",
    "    batch_df = batch_df.withColumn("amount", col("amount").cast("double"))
",
    "
",
    "    # Pattern 2: CHILD - avg txn value < 23 and txns >= 80
",
    "    pattern2 = batch_df.groupBy("customerId", "merchantId", "customerName") \
",
    "        .agg(avg("amount").alias("avg_amount"), count("*").alias("tx_count")) \
",
    "        .filter((col("avg_amount") < 23) & (col("tx_count") >= 80)) \
",
    "        .withColumn("patternId", lit("PatId2")) \
",
    "        .withColumn("ActionType", lit("CHILD")) \
",
    "        .withColumn("YStartTime", current_timestamp()) \
",
    "        .withColumn("detectionTime", current_timestamp()) \
",
    "        .select("YStartTime", "detectionTime", "patternId", "ActionType", "customerName", "merchantId")
",
    "
",
    "    # Write to S3 in chunks of 50
",
    "    pattern2.coalesce(1).write.mode("append").option("maxRecordsPerFile", 50).json("s3://your-s3-bucket/detections/")
"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Trigger Streaming Query
",
    "query = streaming_df.writeStream \
",
    "    .foreachBatch(process_batch) \
",
    "    .option("checkpointLocation", "dbfs:/tmp/checkpoint_detect") \
",
    "    .start()
",
    "
",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
