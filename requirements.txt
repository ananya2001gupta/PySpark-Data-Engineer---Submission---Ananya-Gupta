Assignment Requirements
ðŸ“‚ Input Files
Located at:
ðŸ”— Google Drive Folder

transactions.csv: transaction logs between customers and merchants

CustomerImportance.csv: importance weights for customers and transaction types

âš™ï¸ Mechanism X â€“ Ingest from GDrive to S3
Simulate reading 10,000 rows every second from transactions.csv

Save each chunk into a separate folder on AWS S3

This simulates a real-time feed

âš™ï¸ Mechanism Y â€“ Pattern Detection
Continuously monitor the S3 bucket for new files

Detect and output the following patterns:

ðŸ“Œ Pattern 1: PatId1 - UPGRADE
Merchant has > 50K transactions

Customer is in top 10 percentile (by transaction count)

Customer has low average weight (bottom 10%)

Output action: UPGRADE

ðŸ“Œ Pattern 2: PatId2 - CHILD
Customer has average transaction value < â‚¹23

Has made â‰¥ 80 transactions with the same merchant

Output action: CHILD

ðŸ“Œ Pattern 3: PatId3 - DEI-NEEDED
A merchant has > 100 female customers

Number of female customers < number of male customers

Output action: DEI-NEEDED

ðŸ’¾ PostgreSQL Usage (Mandatory)
Use PostgreSQL to store:

Intermediate counts (e.g. total transactions per merchant)

Running stats (e.g. transaction average per customer)

Detections already performed (to avoid duplicates)

ðŸ’¡ Technical Constraints
Use Databricks with PySpark

Ingest data into AWS S3

Stream detect and output back to S3

Use PostgreSQL for state management

Make necessary assumptions (document them)

You may learn any unfamiliar technology

ðŸ“ Submission Requirements
âœ… GitHub repo with:

Notebooks

PostgreSQL setup scripts

Helper functions

README.md with architecture and how-to-run

âœ… S3 download link to zipped output files

âœ… 4 Loom videos showing:

Code explanation

Running demo

Output & intermediate files

Architecture overview

